{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl0JoW4Eodvi"
   },
   "source": [
    "# **Laboratorio 13:  Airflow **\n",
    "\n",
    "<center><strong>MDS7202: Laboratorio de Programaci贸n Cient铆fica para Ciencia de Datos</strong></center>\n",
    "\n",
    "### **Cuerpo Docente:**\n",
    "\n",
    "- Profesores: Ignacio Meza, Sebasti谩n Tinoco\n",
    "- Auxiliar: Eduardo Moya\n",
    "- Ayudantes: Nicol谩s Ojeda, Melanie Pe帽a, Valentina Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3ypG7Fsodvj"
   },
   "source": [
    "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser谩n revisados\n",
    "\n",
    "- Nombre de alumno 1: Cristopher Urbina H.\n",
    "- Nombre de alumno 2: Joaqu铆n Zamora O.\n",
    "\n",
    "### **Link de repositorio de GitHub:** [Repositorio](https://github.com/CrisU8/MDS7202-Primavera2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_P7PCPTodvk"
   },
   "source": [
    "## Temas a tratar\n",
    "\n",
    "- Construcci贸n de pipelines productivos usando `Airflow`.\n",
    "\n",
    "\n",
    "## Reglas:\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser谩n respondidos por este medio.\n",
    "- Prohibidas las copias.\n",
    "- Pueden usar cualquer material del curso que estimen conveniente.\n",
    "\n",
    "### Objetivos principales del laboratorio\n",
    "\n",
    "- Reconocer los componentes pricipales de `Airflow` y su funcionamiento.\n",
    "- Poner en pr谩ctica la construcci贸n de pipelines de `Airflow`.\n",
    "- Automatizar procesos t铆picos de un proyecto de ciencia de datos mediante `Airflow` y `Docker`.\n",
    "\n",
    "El laboratorio deber谩 ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m谩ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m谩s eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsfK981Uodvk"
   },
   "source": [
    "# **Introducci贸n**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ilM8YDjodvk"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://i.gifer.com/SUFL.gif\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zrLPQNBodvk"
   },
   "source": [
    "Nico, un estudiante del Mag铆ster en Ciencia de Datos, se encuentra en la etapa final de sus estudios. Por un lado, est谩 muy contento por haber llegado tan lejos, pero por otro, no puede evitar sentirse inquieto. Desde que ingres贸 a la universidad, una pregunta lo ha perseguido: 驴qu茅 tan probable es que pueda ser seleccionado en los lugares donde env铆e postulaciones para puestos de trabajo?\n",
    "\n",
    "Esta duda lo mantiene en constante reflexi贸n, especialmente porque sabe que el mercado laboral en Ciencia de Datos es competitivo y exige habilidades no solo t茅cnicas, sino tambi茅n estrat茅gicas para destacar. Sin embargo, Nico actualmente est谩 completamente enfocado en terminar su tesis de mag铆ster y ha tenido que postergar cualquier preparaci贸n espec铆fica para enfrentar el desaf铆o de las postulaciones laborales.\n",
    "\n",
    "Al ver el avance y las habilidades que usted ha demostrado en el curso, Nico decidi贸 proponerle un desaf铆o que le permitir谩 disminuir la incertidumbre sobre su futuro laboral. Inspirado en sus conocimientos, 茅l recolect贸 un conjunto de datos que contiene informaci贸n sobre diversos factores que influyen en las decisiones de contrataci贸n de empresas al seleccionar entre sus postulantes. Este set de datos incluye los siguientes atributos:\n",
    "\n",
    "- Age: Edad del candidato\n",
    "- Gender: Genero del candidato. Male (0), Female (1).\n",
    "- EducationLevel: Mayor nivel educacional alcanzado por el candidato. Licenciatura Tipo 1 (1), Licenciatura Tipo 2 (2), Maestr铆a (3), PhD. (4).\n",
    "- ExperienceYears: A帽os de experiencia profesional.\n",
    "- PreviousCompanies: Numero de compa帽铆as donde el candidato ha trabajado anteriormente.\n",
    "- DistanceFromCompany: Distancia en kilometros entre la residencia del candidato y la compa帽铆a donde postula.\n",
    "- InterviewScore: Puntaje obtenido en la entrevista por el candidato entre 0 a 100.\n",
    "- SkillScore: Puntaje obtenido en evaluaci贸n de habilidades t茅cnicas por el candidato, entre 0 a 100.\n",
    "- PersonalityScore: Puntaje obtenido en pruebas de personalidad del candidato, entre 0 a 100.\n",
    "- RecruitmentStrategy: Estrategia del equipo de reclutamiento. Agresiva (1), Moderada (2), Conservadora (3).\n",
    "\n",
    "Variable a predecir:\n",
    "- HiringDecision: Resultado de la postulaci贸n. No contratado (0), Contratado (1).\n",
    "\n",
    "Su objetivo ser谩 ayudar a Nico a desarrollar un modelo que le permita predecir, basado en estos factores, si un postulante ser谩 contratado o no. Esta herramienta no solo le dar谩 a Nico mayor claridad sobre el impacto de ciertos atributos en la decisi贸n final de contrataci贸n, sino que tambi茅n le permitir谩 aplicar sus conocimientos de Ciencia de Datos para resolver una pregunta que a muchos estudiantes como 茅l les inquieta.\n",
    "\n",
    "Como estudiante del curso Laboratorio de Programaci贸n Cient铆fica para Ciencia de Datos, deber谩 demostrar sus capacidades para preprocesar, analizar y modelar datos, brind谩ndole a Nico una soluci贸n robusta y bien fundamentada para su problem谩tica.\n",
    "\n",
    "`Nota:` El siguiente [enlace](https://www.kaggle.com/datasets/rabieelkharoua/predicting-hiring-decisions-in-recruitment-data/data) contiene el set de datos original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yeh268atodvl"
   },
   "source": [
    "# **1. Pipeline de Predicci贸n Lineal** (30 Puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmB1LTWnodvl"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://c.tenor.com/WvHhQt2UpuAAAAAd/wolf-of-wall-street.gif\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bF1bTY0Modvl"
   },
   "source": [
    "En esta secci贸n buscaremos desplegar un producto utilizando un modelo de clasificaci贸n `Random Forest` para determinar si una persona ser谩 contratada o no en un proceso de selecci贸n. Para ello, comenzaremos preparando un pipeline lineal mediante `Airflow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7MllF4fodvl"
   },
   "source": [
    "## **1.1 Preparando el Pipeline** (15 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1JxaZgModvl"
   },
   "source": [
    "**Primero, aseg煤rese de tener creada las carpetas `dags`, `plugins` y `logs`**.\n",
    "\n",
    "Comenzamos preparando un archivo llamado `hiring_functions.py`, el cual guardar谩 en la carpeta `dags` y debe contener lo siguiente:\n",
    "\n",
    "1. (3 puntos) Una funci贸n llamada `create_folders()` que cree una carpeta, la cual utilice la fecha de ejecuci贸n como nombre. Adicionalmente, dentro de esta carpeta debe crear las siguientes subcarpetas:\n",
    "  - raw\n",
    "  - splits\n",
    "  - models\n",
    "\n",
    "  `Hint`: Puede hacer uso de kwargs para obtener la fecha de ejecuci贸n mediante el DAG. El siguiente [Enlace](https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html) le puede ser 煤til.\n",
    "2. (3 puntos) Una funci贸n llamada `split_data()` que lea el archivo `data_1.csv` de la carepta `raw` y a partir de este, aplique un *hold out*, generando un dataset de entrenamiento y uno de prueba. Luego debe guardar estos nuevos conjuntos de datos en la carpeta `splits`. `Nota:` Utilice un 20% para el conjunto de prueba, mantenga la proporci贸n original en la variable objetivo y fije una semilla.\n",
    "3. (8 puntos) Cree una funci贸n llamada `preprocess_and_train()` que:\n",
    "  - Lea los set de entrenamiento y prueba de la carpeta `splits`.\n",
    "  - Cree y aplique un `Pipeline` con una etapa de preprocesamiento. Utilice `ColumnTransformers` para aplicar las transformaciones que estime convenientes. Puede apoyarse del archivo `data_1_report.html` para justificar cualquier paso del preprocesamiento.\n",
    "  \n",
    "  - A帽ada una etapa de entrenamiento utilizando el modelo `RandomForest`.\n",
    "  \n",
    "  Esta funci贸n **debe crear un archivo `joblib` (an谩logo a `pickle`) con el pipeline entrenado** en la carepta `models`, adem谩s debe **imprimir** el accuracy en el conjunto de prueba y el f1-score de la clase positiva (contratado).\n",
    "3. (1 punto) Incorpore la funci贸n `gradio_interface` en su script, modificando la ruta de acceso a su modelo, de forma que pueda leerlo desde la carepta `models`. Puede realizar modificacioneds adicionales en caso de ser necesario.\n",
    "\n",
    "`NOTA:` Se permite la creaci贸n de funciones auxiliares si lo estiman conveniente."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import joblib\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "# 1. Funci贸n para crear carpetas\n",
    "def create_folders(execution_date):\n",
    "    base_folder = os.path.join(os.getcwd(), f\"output_{execution_date}\")\n",
    "    subfolders = [\"raw\", \"splits\", \"models\"]\n",
    "    os.makedirs(base_folder, exist_ok=True)\n",
    "    for subfolder in subfolders:\n",
    "        os.makedirs(os.path.join(base_folder, subfolder), exist_ok=True)\n",
    "        print(f\"Creating folders for {os.path.join(base_folder, subfolder)}\")\n",
    "\n",
    "\n",
    "\n",
    "# 2. Funci贸n para dividir los datos\n",
    "def split_data(execution_date):\n",
    "    # Define rutas basadas en la fecha de ejecuci贸n\n",
    "    base_path = os.getcwd()\n",
    "    input_path = os.path.join(base_path, f\"output_{execution_date}/raw/data_1.csv\")\n",
    "    split_folder = os.path.join(base_path, f\"output_{execution_date}/splits\")\n",
    "\n",
    "    # Verifica si el archivo de entrada existe\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"No se encontr贸 el archivo: {input_path}\")\n",
    "\n",
    "    # Lee el archivo y divide los datos\n",
    "    data = pd.read_csv(input_path)\n",
    "    if \"HiringDecision\" not in data.columns:\n",
    "        raise ValueError(\"La columna 'HiringDecision' no est谩 en los datos.\")\n",
    "\n",
    "    X = data.drop(columns=[\"HiringDecision\"])\n",
    "    y = data[\"HiringDecision\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # Guarda los conjuntos divididos\n",
    "    pd.concat([X_train, y_train], axis=1).to_csv(os.path.join(split_folder, \"train.csv\"), index=False)\n",
    "    pd.concat([X_test, y_test], axis=1).to_csv(os.path.join(split_folder, \"test.csv\"), index=False)\n",
    "    print(f\"Archivos guardados en {split_folder}\")\n",
    "\n",
    "\n",
    "\n",
    "# 3. Funci贸n para preprocesar y entrenar modelo\n",
    "def preprocess_and_train(execution_date):\n",
    "    # Leer datasets\n",
    "    base_path = os.getcwd()\n",
    "    train_path = os.path.join(base_path, f\"output_{execution_date}/splits/train.csv\")\n",
    "    test_path = os.path.join(base_path, f\"output_{execution_date}/splits/test.csv\")\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "\n",
    "    X_train = train_data.drop(columns=[\"HiringDecision\"])\n",
    "    y_train = train_data[\"HiringDecision\"]\n",
    "    X_test = test_data.drop(columns=[\"HiringDecision\"])\n",
    "    y_test = test_data[\"HiringDecision\"]\n",
    "\n",
    "    # Preprocesamiento\n",
    "    numeric_features = [\"Age\", \"ExperienceYears\", \"DistanceFromCompany\", \"InterviewScore\", \"SkillScore\",\n",
    "                        \"PersonalityScore\"]\n",
    "    categorical_features = [\"Gender\", \"EducationLevel\", \"PreviousCompanies\", \"RecruitmentStrategy\"]\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
    "    categorical_transformer = Pipeline(steps=[(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Pipeline de modelo\n",
    "    model = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=42)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Entrenar modelo\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluaci贸n\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"F1 Score (Positive Class): {f1:.4f}\")\n",
    "\n",
    "    # Guardar modelo entrenado\n",
    "    model_path = f\"output_{execution_date}/models/hiring_model.joblib\"\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "\n",
    "def predict(file,model_path):\n",
    "\n",
    "    pipeline = joblib.load(model_path)\n",
    "    input_data = pd.read_json(file)\n",
    "    predictions = pipeline.predict(input_data)\n",
    "    print(f'La prediccion es: {predictions}')\n",
    "    labels = [\"No contratado\" if pred == 0 else \"Contratado\" for pred in predictions]\n",
    "\n",
    "    return {'Predicci贸n': labels[0]}\n",
    "\n",
    "\n",
    "def gradio_interface(execution_date):\n",
    "\n",
    "    model_path= f'output_{execution_date}/models/hiring_model.joblib'\n",
    "\n",
    "    interface = gr.Interface(\n",
    "        fn=lambda file: predict(file, model_path),\n",
    "        inputs=gr.File(label=\"Sube un archivo JSON\"),\n",
    "        outputs=\"json\",\n",
    "        title=\"Hiring Decision Prediction\",\n",
    "        description=\"Sube un archivo JSON con las caracter铆sticas de entrada para predecir si Nico ser谩 contratado o no.\"\n",
    "    )\n",
    "    interface.launch(share=True)\n"
   ]
  },
  {
   "metadata": {
    "id": "lTKOj1hfodvm"
   },
   "cell_type": "markdown",
   "source": "## **1.2 Creando Nuestro DAG** (15 puntos)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkEZcEh4odvm"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExMzNjd3hxOWIzZjhwZDc5NnJwZzZodnNrbWI5cGtjY2VwZjI0eDdnOSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/Dh5q0sShxgp13DwrvG/giphy.webp\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-MTaxTgodvm"
   },
   "source": [
    "Con las funciones del pipeline ya creadas, ahora vamos a proceder a crear un Directed Acyclic Graph (DAG). Para ello, se le pide lo siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-yUak2Rodvm"
   },
   "source": [
    "- (10 puntos) Cree un segundo archivo llamado `dag_lineal.py` y guardelo en la carpeta dags. Este script debe seguir la siguiente estructura (Ver imagen de referencia):\n",
    "\n",
    "    0. Inicialice un DAG con fecha de inicio el 1 de octubre de 2024, ejecuci贸n manual y **sin backfill**. Asigne un `dag_id` que pueda reconocer facilmente, como `hiring_lineal`, etc.\n",
    "    1. Debe comenzar con un marcador de posici贸n que indique el inicio del pipeline.\n",
    "    2. Cree una carpeta correspondiente a la ejecuci贸n del pipeline y cree las subcarpetas `raw`, `splits` y `models` mediante la funci贸n `create_folders()`.\n",
    "    3. Debe descargar el archivo `data_1.csv` del siguiente [enlace](https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv). Debe guardar el archivo en la carpeta raw de la ejecuci贸n correspondiente.`Hint:` Le puede ser 煤til el comando `curl -o <path de guardado> <enlace con los datos>`.\n",
    "    4. Debe aplicar un hold out mediante la funci贸n `split_data()` de su archivo creado en la subsecci贸n anterior.\n",
    "    5. Debe aplicar el preprocesamiento y el entrenamiento del modelo mediante la funci贸n `preprocess_and_train()`.\n",
    "    6. Finalmente, debe montar una interfaz en gradio donde pueda cargar un archivo ``json``.\n",
    "\n",
    "\n",
    "- (3 puntos) Cree un `DockerFile` para montar un contenedor que contenga Airflow. Adicionalmente, cree una carpeta llamada dags donde guardar谩 el script.py creado anteriormente.\n",
    "\n",
    "    `Nota:` Para la imagen, se recomienda utilizar python 3.10-slim. Adicionalmente, puede instalar `curl` mediante la siguiente linea de c贸digo: `RUN apt-get update && apt-get install -y curl`.\n",
    "\n",
    "- Construya el contenedor en Docker y acceda a la aplicaci贸n web de Airflow mediante el siguiente [enlace](http://localhost:8080/). Inicie sesi贸n, acceda al DAG creado y ejecute de forma manual su pipeline.\n",
    "\n",
    "- (2 puntos) Acceda a la URL p煤blica de Gradio e ingrese el archivo `nico_data.json` a su modelo. 驴Que predicci贸n entreg贸 el modelo para Nico? Adjunte im谩genes de su resultado. `Hint:` Puede acceder a los `logs` para obtener los prints y la URL p煤blica.\n",
    "\n",
    "`Hint:` Recuerde que puede entregar `kwargs` a sus funciones, como por ejemplo la fecha de ejecuci贸n `ds`.\n",
    "\n",
    "**Para esta secci贸n, debe adjuntar todos los scripts creados junto a su notebook en la entrega, ya que ser谩n ejecutados para validar el funcionamiento. Para justificar sus respuestas, adicionaslmente puede utilizar im谩genes de apoyo, como screenshots.**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "DAG de referencia:\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://drive.google.com/uc?id=1iwDgECZfFeWq1dl433tMa6_3CNF9cn1L\" width=\"1200\">\n",
    "</p>\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "tiMTgQfJpuIv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckzDqsF4odvn"
   },
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "from hiring_functions import create_folders, split_data, preprocess_and_train, gradio_interface\n",
    "\n",
    "\n",
    "# Inicializaci贸n del DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "# Definici贸n del DAG\n",
    "with DAG(\n",
    "    dag_id=\"hiring_lineal\",\n",
    "    default_args=default_args,\n",
    "    description=\"Pipeline para predicci贸n de contrataci贸n\",\n",
    "    schedule_interval=None,  # Ejecuci贸n manual\n",
    "    start_date=datetime(2024, 10, 1),\n",
    "    catchup=False,  # Sin backfill\n",
    "    params={}\n",
    ") as dag:\n",
    "\n",
    "    # 1. Marcador de inicio\n",
    "    start = DummyOperator(task_id=\"start_pipeline\")\n",
    "\n",
    "    # 2. Crear carpetas\n",
    "    create_folders_task = PythonOperator(\n",
    "        task_id=\"create_folders\",\n",
    "        python_callable=create_folders,\n",
    "        op_kwargs={\"execution_date\": \"{{ ds }}\"},\n",
    "    )\n",
    "\n",
    "    # 3. Descargar datos\n",
    "    download_data_task = BashOperator(\n",
    "        task_id='download_dataset',\n",
    "        bash_command=(\n",
    "            \"curl -o /root/airflow/output_{{ ds }}/raw/data_1.csv \"\n",
    "            \"https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # 4. Aplicar hold out\n",
    "    split_data_task = PythonOperator(\n",
    "        task_id=\"split_data\",\n",
    "        python_callable=split_data,\n",
    "        provide_context=True,\n",
    "        op_kwargs={\"execution_date\": \"{{ ds }}\"}\n",
    "    )\n",
    "\n",
    "    # 5. Preprocesar y entrenar\n",
    "    preprocess_and_train_task = PythonOperator(\n",
    "        task_id=\"preprocess_and_train\",\n",
    "        python_callable=preprocess_and_train,\n",
    "        provide_context=True,\n",
    "        op_kwargs={\"execution_date\": \"{{ ds }}\"}\n",
    "    )\n",
    "\n",
    "    # 6. Generar interfaz con Gradio\n",
    "    gradio_interface_task = PythonOperator(\n",
    "        task_id=\"gradio_interface\",\n",
    "        python_callable=gradio_interface,\n",
    "        provide_context=True,\n",
    "        op_kwargs={\"execution_date\": \"{{ ds }}\"},\n",
    "    )\n",
    "\n",
    "    # 7. Finalizar\n",
    "    end = DummyOperator(task_id=\"end_pipeline\")\n",
    "\n",
    "    # Definici贸n del flujo de tareas\n",
    "    start >> create_folders_task >> download_data_task >> split_data_task >> preprocess_and_train_task >> gradio_interface_task >> end\n",
    " "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![](dag_lineal.png)\n",
    "![](img.png)\n",
    "![](img_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **2. Paralelizando el Pipeline** (30 puntos)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.tenor.com/vDv7mn58skcAAAAM/clapping.gif\" width=\"300\">\n",
    "</p>\n",
    "\n"
   ],
   "metadata": {
    "id": "KqBlHcBQpXJb"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Al ver los resultados obtenidos, Nico queda muy contento con el clasificador. Sin embargo, le aparecen algunas dudas respecto al funcionamiento del pipeline. Primero le comenta que es posible que en un futuro tenga nuevos datos que podr铆an ser 煤tiles para realizar nuevos entrenamientos, por lo que ser铆a ideal si este pipeline se fuera ejecutando de forma peri贸dica y no de forma manual. Adem谩s, Nico le menciona que le gustar铆a explorar el desempe帽o de otros modelos adem谩s de `Random Forest`, de forma que el pipeline seleccione de forma autom谩tica el modelo con mejor desempe帽o para luego hacer la predicci贸n de Nico."
   ],
   "metadata": {
    "id": "SoQaVOeiqO_R"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **2.1 Preparando un Nuevo Pipeline** (15 puntos)"
   ],
   "metadata": {
    "id": "9mGPMg0ur-wR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://i.makeagif.com/media/7-07-2015/oH6WRw.gif\" width=\"400\">\n",
    "</p>"
   ],
   "metadata": {
    "id": "fpU81VCRr-Hr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "De acuerdo a lo que le coment贸 Nico, usted decide crear un nuevo script con las funciones que utilizar谩 su pipeline. Por ende, dentro de la carpeta `dags`, usted crear谩 el archivo `hiring_dynamic_functions.py` el cual debe contener:\n",
    "\n",
    "1. (2 puntos) Una funci贸n llamada `create_folders()` que cree una carpeta, la cual utilice la fecha de ejecuci贸n como nombre. Adicionalmente, dentro de esta carpeta debe crear las siguientes subcarpetas:\n",
    "  - raw\n",
    "  - preprocessed\n",
    "  - splits\n",
    "  - models\n",
    "2. (2 puntos) Una funci贸n llamada `load_ands_merge()` que lea desde la carpeta `raw` los archivos `data_1.csv`y `data_2.csv` en caso de estar disponible. Luego concatene estos y genere un nuevo archivo resultante, guard谩ndolo en la carpeta `preprocessed`.\n",
    "\n",
    "3. (2 puntos) Una funci贸n llamada `split_data()` que lea la data guardada en la carpeta `preprocessed` y realice un hold out sobre esta data. Esta funci贸n debe crear un conjunto de entrenamiento y uno de prueba. Mantenga una semilla y 20% para el conjunto de prueba. Guarde los conjuntos resultantes en la carpeta `splits`.\n",
    "\n",
    "4. (6 puntos) Una funci贸n llamada `train_model()` que reciba un modelo de clasificaci贸n.\n",
    "    - La funci贸n debe comenzar leyendo el conjunto de entrenamiento desde la carpeta `spits`.\n",
    "    - Esta debe crear y aplicar un `Pipeline` con una etapa de preprocesamiento. Utilice `ColumnTransformers` para aplicar las transformaciones que estime convenientes.\n",
    "    - A帽ada una etapa de entrenamiento utilizando un modelo que ingrese a la funci贸n.\n",
    "  \n",
    "  Esta funci贸n **debe crear un archivo joblib con el pipeline entrenado**. Guarde el modelo con un nombre que le permita una facil identificaci贸n dentro de la carpeta `models`.\n",
    "\n",
    "5. (3 puntos) Una funci贸n llamada `evaluate_models()` que reciba sus modelos entrenados desde la carpeta `models`, eval煤e su desempe帽o mediante `accuracy` en el conjunto de prueba y seleccione el mejor modelo obtenido. Luego guarde el mejor modelo como archivo `.joblib`. Su funci贸n debe imprimir el nombre del modelo seleccionado y el accuracy obtenido."
   ],
   "metadata": {
    "id": "7KcXuS6bsZAw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from glob import glob\n",
    "\n",
    "# 1. Funci贸n para crear carpetas\n",
    "def create_folders(execution_date):\n",
    "    base_folder = os.path.join(os.getcwd(), f\"output_{execution_date}\")\n",
    "    subfolders = [\"raw\", \"splits\", \"models\",\"preprocessed\"]\n",
    "    os.makedirs(base_folder, exist_ok=True)\n",
    "    for subfolder in subfolders:\n",
    "        os.makedirs(os.path.join(base_folder, subfolder), exist_ok=True)\n",
    "        print(f\"Creating folders for {os.path.join(base_folder, subfolder)}\")\n",
    "\n",
    "# 2. Leer y concatenar archivos\n",
    "def load_and_merge(execution_date):\n",
    "    \"\"\"\n",
    "    Lee los archivos `data_1.csv` y `data_2.csv` desde la carpeta `raw`,\n",
    "    los concatena si ambos existen, y guarda el resultado en `preprocessed`.\n",
    "    \"\"\"\n",
    "    # Definir rutas de las carpetas\n",
    "    raw_folder = f\"output_{execution_date}/raw\"\n",
    "    preprocessed_folder = f\"output_{execution_date}/preprocessed\"\n",
    "\n",
    "\n",
    "    # Definir los archivos a buscar\n",
    "    data_files = [os.path.join(raw_folder, f\"data_{i}.csv\") for i in range(1, 3)]\n",
    "\n",
    "    # Leer los archivos que existen\n",
    "    data_frames = []\n",
    "    for file in data_files:\n",
    "        if os.path.exists(file):\n",
    "            print(f\"Leyendo archivo: {file}\")\n",
    "            data_frames.append(pd.read_csv(file))\n",
    "        else:\n",
    "            print(f\"Archivo no encontrado: {file}\")\n",
    "\n",
    "    if not data_frames:\n",
    "        raise FileNotFoundError(\"No se encontraron archivos `data_1.csv` o `data_2.csv` en la carpeta `raw`.\")\n",
    "\n",
    "    # Concatenar DataFrames\n",
    "    combined_data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    # Guardar los datos combinados\n",
    "    output_file = os.path.join(preprocessed_folder, \"combined_data.csv\")\n",
    "    combined_data.to_csv(output_file, index=False)\n",
    "    print(f\"Datos combinados guardados en: {output_file}\")\n",
    "\n",
    "# 3. Dividir datos\n",
    "def split_data(execution_date):\n",
    "    \"\"\"\n",
    "    Divide el archivo `combined_data.csv` en conjuntos de entrenamiento y prueba (80%-20%).\n",
    "    Guarda los conjuntos en la carpeta `splits`.\n",
    "    \"\"\"\n",
    "    preprocessed_folder = f\"output_{execution_date}/preprocessed\"\n",
    "    splits_folder = f\"output_{execution_date}/splits\"\n",
    "\n",
    "    # Leer datos preprocesados\n",
    "    data_path = os.path.join(preprocessed_folder, \"combined_data.csv\")\n",
    "    data = pd.read_csv(data_path)\n",
    "\n",
    "    # Dividir datos\n",
    "    X = data.drop(columns=[\"HiringDecision\"])\n",
    "    y = data[\"HiringDecision\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # Guardar datos\n",
    "    pd.concat([X_train, y_train], axis=1).to_csv(os.path.join(splits_folder, \"train.csv\"), index=False)\n",
    "    pd.concat([X_test, y_test], axis=1).to_csv(os.path.join(splits_folder, \"test.csv\"), index=False)\n",
    "\n",
    "# 4. Entrenar modelo\n",
    "def train_model(model, execution_date):\n",
    "    \"\"\"\n",
    "    Entrena un modelo de clasificaci贸n utilizando los datos de entrenamiento.\n",
    "    Guarda el modelo entrenado en la carpeta `models`.\n",
    "    \"\"\"\n",
    "    splits_folder = f\"output_{execution_date}/splits\"\n",
    "    models_folder = f\"output_{execution_date}/models\"\n",
    "\n",
    "    # Leer conjunto de entrenamiento\n",
    "    train_path = os.path.join(splits_folder, \"train.csv\")\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    X_train = train_data.drop(columns=[\"HiringDecision\"])\n",
    "    y_train = train_data[\"HiringDecision\"]\n",
    "\n",
    "    # Configurar preprocesamiento\n",
    "    numeric_features = [\"Age\", \"ExperienceYears\", \"DistanceFromCompany\", \"InterviewScore\", \"SkillScore\", \"PersonalityScore\"]\n",
    "    categorical_features = [\"Gender\", \"EducationLevel\", \"PreviousCompanies\", \"RecruitmentStrategy\"]\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
    "    categorical_transformer = Pipeline(steps=[(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Crear pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", model),\n",
    "    ])\n",
    "\n",
    "    # Entrenar modelo\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Guardar modelo entrenado\n",
    "    model_name = type(model).__name__\n",
    "    joblib.dump(pipeline, os.path.join(models_folder, f\"{model_name}.joblib\"))\n",
    "\n",
    "# 5. Evaluar modelos\n",
    "def evaluate_models(execution_date):\n",
    "    \"\"\"\n",
    "    Eval煤a los modelos entrenados y selecciona el mejor basado en `accuracy`.\n",
    "    Guarda el mejor modelo como `best_model.joblib` en la carpeta `models`.\n",
    "    \"\"\"\n",
    "    splits_folder = f\"output_{execution_date}/splits\"\n",
    "    models_folder = f\"output_{execution_date}/models\"\n",
    "\n",
    "    # Leer conjunto de prueba\n",
    "    test_path = os.path.join(splits_folder, \"test.csv\")\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    X_test = test_data.drop(columns=[\"HiringDecision\"])\n",
    "    y_test = test_data[\"HiringDecision\"]\n",
    "\n",
    "    # Evaluar modelos\n",
    "    model_files = glob(os.path.join(models_folder, \"*.joblib\"))\n",
    "    if not model_files:\n",
    "        raise FileNotFoundError(\"No se encontraron modelos en la carpeta `models`.\")\n",
    "\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    best_model_name = \"\"\n",
    "\n",
    "    for model_file in model_files:\n",
    "        model = joblib.load(model_file)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        model_name = os.path.basename(model_file).replace(\".joblib\", \"\")\n",
    "        print(f\"Modelo: {model_name}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_model = model\n",
    "            best_model_name = model_name\n",
    "\n",
    "    # Guardar mejor modelo\n",
    "    if best_model:\n",
    "        joblib.dump(best_model, os.path.join(models_folder, \"best_model.joblib\"))\n",
    "        print(f\"Mejor modelo: {best_model_name}, Accuracy: {best_accuracy:.4f}\")\n",
    "    else:\n",
    "        raise ValueError(\"No se pudo determinar el mejor modelo.\")"
   ],
   "metadata": {
    "id": "KnX61hxjW9rI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **2.2 Componiendo un nuevo DAG** (15 puntos)"
   ],
   "metadata": {
    "id": "RUYkXWcZJz3b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/977552ab-0b55-4118-9948-06f6386474da_text.gif\" width=\"400\">\n",
    "</p>"
   ],
   "metadata": {
    "id": "ak7uL9YXJ6Xj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Con las nuevas funciones, vamos a crear nuestro nuevo DAG. Para ello, cree un nuevo script en la carpeta `dags`, llamandolo `dag_dynamic.py`. Este script debe contener la siguiente estructura:\n",
    "\n",
    "1. (1 punto) Inicialice un DAG con fecha de inicio el 1 de octubre de 2024, el cual se debe ejecutar el d铆a 5 de cada mes a las 15:00 UTC. Utilice un `dag_id` interpretable para identificar f谩cilmente. **Habilite el backfill** para que pueda ejecutar tareas programadas desde fechas pasadas.\n",
    "2. (1 punto) Comience con un marcador de posici贸n que indique el inicio del pipeline.\n",
    "3. (2 puntos) Cree una carpeta correspondiente a la ejecuci贸n del pipeline y cree las subcarpetas `raw`, `preprocessed`, `splits` y `models` mediante la funci贸n `create_folders()`.\n",
    "4. (2 puntos) Implemente un `Branching`que siga la siguiente l贸gica:\n",
    "  - Fechas previas al 1 de noviembre de 2024: Se descarga solo `data_1.csv`\n",
    "  - Desde el 1 de noviembre del 2024: descarga `data_1.csv` y `data_2.csv`.\n",
    "  En el siguiente [enlace](https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_2.csv) puede descargar `data_2.csv`.\n",
    "5. (1 punto) Cree una tarea que concatene los datasets disponibles mediante la funci贸n `load_and_merge()`. Configure un `Trigger` para que la tarea se ejecute si encuentra disponible **como m铆nimo** uno de los archivos.\n",
    "6. (1 punto) Aplique el hold out al dataset mediante la funci贸n `split_data()`, obteniendo un conjunto de entrenamiento y uno de prueba.\n",
    "7. (2 puntos) Realice 3 entrenamientos en paralelo:\n",
    "  - Un modelo Random Forest.\n",
    "  - 2 modelos a elecci贸n.\n",
    "  Aseg煤rese de guardar sus modelos entrenados con nombres distintivos. Utilice su funci贸n `train_model()` para ello.\n",
    "8. (2 puntos)Mediante la funci贸n `evaluate_models()`, eval煤e los modelos entrenados, registrando el accuracy de cada modelo en el set de prueba. Luego debe imprimir el mejor modelo seleccionado y su respectiva m茅trica. Configure un `Trigger` para que la tarea se ejecute solamente si los 3 modelos fueron entrenados y guardados.\n",
    "\n",
    "`Hint:` Recuerde que puede entregar `kwargs` a sus funciones, como por ejemplo la fecha de ejecuci贸n `ds`.\n",
    "\n",
    "Una vez creado el script, vuelva a construir el contenedor en Docker, acceda a la aplicaci贸n web de Airflow, ejecute su pipeline y muestre sus resultados. Adjunte im谩genes que ayuden a mostrar el proceso y sus resultados.\n",
    "\n",
    "Adicionalmente, responda (1 c/u):\n",
    "\n",
    "- 驴Cual es el accuracy de cada modelo en la ejecuci贸n de octubre? 驴Se obtienen los mismos resultados a partir de Noviembre?\n",
    "- Analice como afect贸 el a帽adir datos a sus modelos mediante el desempe帽o del modelo y en costo computacional.\n",
    "- Muestre el esquema de su DAG ejecutado en octubre y en noviembre.\n",
    "\n",
    "\n",
    "`Nota:` Para esta secci贸n no debe implementar la tarea en gradio, solamente se espera determinar el mejor modelo y comparar el desempe帽o obtenido.\n",
    "\n",
    "**IMPORTANTE: Para esta secci贸n, debe adjuntar todos los scripts creados junto a su notebook en la entrega, ya que ser谩n ejecutados para validar el funcionamiento. Para justificar sus respuestas, adicionaslmente puede utilizar im谩genes de apoyo, como screenshots.**"
   ],
   "metadata": {
    "id": "NbE6mu20LfWd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "from datetime import datetime\n",
    "from hiring_dynamic_functions import create_folders, load_and_merge, split_data, train_model, evaluate_models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def decide_which_datasets(**kwargs):\n",
    "    \"\"\"\n",
    "    Decide qu茅 datasets descargar basado en la fecha de ejecuci贸n.\n",
    "    \"\"\"\n",
    "    # Obtener execution_date del contexto\n",
    "    execution_date = kwargs['ds']\n",
    "\n",
    "    # Crear fecha l铆mite\n",
    "    cutoff_date = '2024-11-01'\n",
    "\n",
    "    # Validar y comparar las fechas\n",
    "    if execution_date < cutoff_date:\n",
    "        print(f\"Execution date {execution_date} es anterior a {cutoff_date}. Descargando data_1.csv.\")\n",
    "        return \"download_data_1\"\n",
    "    else:\n",
    "        print(f\"Execution date {execution_date} es igual o posterior a {cutoff_date}. Descargando data_1.csv y data_2.csv.\")\n",
    "        return \"download_data_1_and_2\"\n",
    "\n",
    "\n",
    "# Configuraci贸n del DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"hiring_dynamic_pipeline\",\n",
    "    default_args=default_args,\n",
    "    description=\"Pipeline din谩mico para entrenamiento de modelos de contrataci贸n\",\n",
    "    schedule_interval=\"0 15 5 * *\",  # Ejecutar el d铆a 5 de cada mes a las 15:00 UTC\n",
    "    start_date=datetime(2024, 10, 1),\n",
    "    catchup=True,  # Habilitar backfill\n",
    ") as dag:\n",
    "\n",
    "    # Marcador de inicio\n",
    "    start = DummyOperator(task_id=\"start_pipeline\")\n",
    "\n",
    "    # Crear carpetas\n",
    "    create_folders_task = PythonOperator(\n",
    "        task_id=\"create_folders\",\n",
    "        python_callable=create_folders,\n",
    "        op_kwargs={\"execution_date\": \"{{ ds }}\"},\n",
    "    )\n",
    "\n",
    "    branch_task = BranchPythonOperator(\n",
    "        task_id=\"decide_datasets\",\n",
    "        python_callable=decide_which_datasets,\n",
    "        provide_context=True,  # Para acceder a execution_date\n",
    "    )\n",
    "\n",
    "    # Descargar data_1.csv\n",
    "    download_data_1_task = BashOperator(\n",
    "        task_id='download_data_1',\n",
    "        bash_command=(\n",
    "            \"curl -o /root/airflow/output_{{ ds }}/raw/data_1.csv \"\n",
    "            \"https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Descargar data_1.csv y data_2.csv\n",
    "    download_data_1_and_2_task = BashOperator(\n",
    "        task_id='download_data_1_and_2',\n",
    "        bash_command=(\n",
    "             \"curl -o /root/airflow/output_{{ ds }}/raw/data_1.csv \"\n",
    "        \"https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv && \"\n",
    "        \"curl -o /root/airflow/output_{{ ds }}/raw/data_2.csv \"\n",
    "        \"https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_2.csv\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "    # Concatenar datasets\n",
    "    load_and_merge_task = PythonOperator(\n",
    "        task_id=\"load_and_merge\",\n",
    "        python_callable=load_and_merge,\n",
    "        op_kwargs={\"execution_date\": \"{{ ds }}\"},\n",
    "        trigger_rule=TriggerRule.ONE_SUCCESS,  # Ejecutar si al menos una rama del Branch fue exitosa\n",
    "    )\n",
    "\n",
    "    # Aplicar hold-out\n",
    "    split_data_task = PythonOperator(\n",
    "        task_id=\"split_data\",\n",
    "        python_callable=split_data,\n",
    "        op_kwargs={\"execution_date\": \"{{ ds }}\"},\n",
    "    )\n",
    "\n",
    "    # Entrenamiento de modelos\n",
    "    train_rf_task = PythonOperator(\n",
    "        task_id=\"train_random_forest\",\n",
    "        python_callable=train_model,\n",
    "        op_kwargs={\"model\": RandomForestClassifier(random_state=42), \"execution_date\": \"{{ ds }}\"},\n",
    "    )\n",
    "    train_logreg_task = PythonOperator(\n",
    "        task_id=\"train_logistic_regression\",\n",
    "        python_callable=train_model,\n",
    "        op_kwargs={\"model\": LogisticRegression(max_iter=1000), \"execution_date\": \"{{ ds }}\"},\n",
    "    )\n",
    "    train_svc_task = PythonOperator(\n",
    "        task_id=\"train_svc\",\n",
    "        python_callable=train_model,\n",
    "        op_kwargs={\"model\": SVC(), \"execution_date\": \"{{ ds }}\"},\n",
    "    )\n",
    "\n",
    "    # Evaluar modelos\n",
    "    evaluate_models_task = PythonOperator(\n",
    "        task_id=\"evaluate_models\",\n",
    "        python_callable=evaluate_models,\n",
    "        op_kwargs={\"execution_date\": \"{{ ds }}\"},\n",
    "        trigger_rule=TriggerRule.ALL_SUCCESS,  # Ejecutar solo si todos los modelos fueron entrenados\n",
    "    )\n",
    "\n",
    "    # Marcador de fin\n",
    "    end = DummyOperator(task_id=\"end_pipeline\")\n",
    "\n",
    "    # Flujo de tareas\n",
    "    start >> create_folders_task >> branch_task\n",
    "    branch_task >> [download_data_1_task, download_data_1_and_2_task]\n",
    "    [download_data_1_task, download_data_1_and_2_task] >> load_and_merge_task >> split_data_task\n",
    "    split_data_task >> [train_rf_task, train_logreg_task, train_svc_task] >> evaluate_models_task >> end"
   ],
   "metadata": {
    "id": "cMgK2sKTYJji"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Metricas octubre\n",
    "![](metricas_octubree.png)\n",
    "\n",
    "### Metricas a noviembre\n",
    "![](metricas_noviembre.png)\n",
    "\n",
    ">Durante noviembre, las m茅tricas muestran una tendencia general al alza, con la excepci贸n de SVC, que mantiene un comportamiento distinto.\n",
    ">\n",
    ">En cuanto a los tiempos de ejecuci贸n, estos fueron muy similares, con diferencias menores a 5 segundos entre ellos. Sin embargo, SVC destaca por un incremento en el tiempo de entrenamiento en comparaci贸n con octubre, as铆 como un aumento en el tiempo de evaluaci贸n. Ambos incrementos est谩n directamente relacionados con la mayor cantidad de datos procesados durante este per铆odo.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Ejecuci贸n de octubre\n",
    "![Ejecucion de octubre](dag_octubre.png)\n",
    "### Ejecucion de Noviembre\n",
    "![Ejecucion de noviembre](dag_noviembre.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusi贸n\n",
    "Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por el foro de U-cursos o por correo.\n",
    "\n",
    "<center>\n",
    "<img src =\"https://media0.giphy.com/media/W12WAzuqod9VS/200w.gif?cid=6c09b952gekq3fm1no1ttwcvgm9oj3khbm6yxbe6qwnx3nad&ep=v1_gifs_search&rid=200w.gif&ct=g\" width = 400 />\n"
   ],
   "metadata": {
    "id": "wrmM65RIRrgm"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab_MDS_Primavera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
