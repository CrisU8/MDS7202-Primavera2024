{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Laboratorio 11: LLM y Agentes Autónomos 🤖**\n",
    "\n",
    "MDS7202: Laboratorio de Programación Científica para Ciencia de Datos"
   ],
   "metadata": {
    "id": "PyPTffTLug7i"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Cuerpo Docente:**\n",
    "\n",
    "- Profesores: Ignacio Meza, Sebastián Tinoco\n",
    "- Auxiliar: Eduardo Moya\n",
    "- Ayudantes: Nicolás Ojeda, Melanie Peña, Valentina Rojas"
   ],
   "metadata": {
    "id": "5pbWVyntzbvL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Equipo: SUPER IMPORTANTE - notebooks sin nombre no serán revisados**\n",
    "\n",
    "- Nombre de alumno 1: Cristopher Urbina H.\n",
    "- Nombre de alumno 2: Joaquín Zamora O.\n",
    "\n"
   ],
   "metadata": {
    "id": "dy6ikgVYzghB"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### **Link de repositorio de GitHub:** [Repositorio](https://github.com/CrisU8/MDS7202-Primavera2024)\n",
   "metadata": {
    "id": "iMJ-owchzjFf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Temas a tratar**\n",
    "\n",
    "- Reinforcement Learning\n",
    "- Large Language Models\n",
    "\n",
    "## **Reglas:**\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
    "- Prohibidas las copias.\n",
    "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
    "\n",
    "### **Objetivos principales del laboratorio**\n",
    "\n",
    "El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante más eficientes que los iteradores nativos sobre DataFrames."
   ],
   "metadata": {
    "id": "WUuwsXrKzmkK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **1. Reinforcement Learning (2.0 puntos)**\n",
    "\n",
    "En esta sección van a usar métodos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
   ],
   "metadata": {
    "id": "0hmHHQ9BuyAG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -qqq gymnasium stable_baselines3\n",
    "!pip install -qqq swig\n",
    "!pip install -qqq gymnasium[box2d]"
   ],
   "metadata": {
    "id": "gOcejYb6uzOO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6bb6c201-bbcc-433c-9794-8cb37280b75c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/958.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m276.5/958.1 kB\u001B[0m \u001B[31m8.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m952.3/958.1 kB\u001B[0m \u001B[31m18.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m958.1/958.1 kB\u001B[0m \u001B[31m13.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m183.9/183.9 kB\u001B[0m \u001B[31m12.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m16.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m374.4/374.4 kB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Building wheel for box2d-py (setup.py) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **1.1 Blackjack (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "La idea de esta subsección es que puedan implementar métodos de RL y así generar una estrategia para jugar el clásico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de código transforma las observaciones del ambiente a `np.array`:\n"
   ],
   "metadata": {
    "id": "qBPet_Mq8dX9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np\n",
    "\n",
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)"
   ],
   "metadata": {
    "id": "LpZ8bBKk9ZlU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **1.1.1 Descripción de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripción sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
   ],
   "metadata": {
    "id": "ZJ6J1_-Y9nHO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 1. **Estados (Observaciones):**\n",
    "La información que el agente recibe del entorno:\n",
    "- **Suma del jugador:** \\(4 <= 21\\).\n",
    "- **Carta visible del dealer:** \\(1 <= 10\\).\n",
    "- **As usable:** \\(0\\) o \\(1\\).\n",
    "\n",
    "El espacio de estados se representa como un vector:\n",
    "\\[\n",
    "(\\text{Player's sum}, \\text{Dealer's card}, \\text{Usable ace})\n",
    "\\]\n",
    "\n",
    "### 2. **Acciones:**\n",
    "Conjunto de decisiones posibles:\n",
    "- \\(0\\): **Stick** (no pedir más cartas).\n",
    "- \\(1\\): **Hit** (pedir una carta adicional).\n",
    "\n",
    "El espacio de acciones es discreto con dos opciones (\\(\\{0, 1\\}\\)).\n",
    "\n",
    "### 3. **Recompensas:**\n",
    "Feedback otorgado al agente por ejecutar una acción:\n",
    "- \\(+1.5\\): Ganar con un blackjack natural (si se permite esta regla).\n",
    "- \\(+1\\): Ganar.\n",
    "- \\(0\\): Empatar.\n",
    "- \\(-1\\): Perder (por bust o puntuación menor que la del dealer).\n",
    "\n",
    "### Dinámica del MDP\n",
    "- **Transiciones**:\n",
    "  - Las acciones afectan el estado del jugador (suma de cartas y condición de bust).\n",
    "  - El dealer actúa automáticamente siguiendo reglas fijas cuando el jugador elige \"stick\".\n",
    "- **Probabilidades**:\n",
    "  - Las cartas son seleccionadas al azar (baraja infinita).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "G5i1Wt1p770x"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 5000 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política? ¿Cómo podría interpretar las recompensas obtenidas?"
   ],
   "metadata": {
    "id": "pmcX6bRC9agQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Simulate 5000 games with random actions\n",
    "n_simulations = 5000\n",
    "rewards = []\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Take a random action\n",
    "        _, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward  # Accumulate reward for the episode\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# Calculate mean and standard deviation of rewards\n",
    "mean_reward = np.mean(rewards)\n",
    "std_reward = np.std(rewards)\n",
    "\n",
    "print(f\"Mean Reward: {mean_reward}\")\n",
    "print(f\"Standard Deviation of Rewards: {std_reward}\")"
   ],
   "metadata": {
    "id": "9p2PrLLR9yju",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c1bf5edd-ed73-4667-b99c-b421c1f32d53"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean Reward: -0.3848\n",
      "Standard Deviation of Rewards: 0.8977354621490676\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
   ],
   "metadata": {
    "id": "LEO_dY4x_SJu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Wrapper para aplanar el espacio de observación\n",
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        # Cambiar el espacio de observación a Box\n",
    "        self.observation_space = Box(low=0, high=32, shape=(3,), dtype=np.float32)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        # Aplanar las observaciones a un vector\n",
    "        return np.array(obs, dtype=np.float32)\n",
    "\n",
    "# Crear el entorno con el wrapper aplicado\n",
    "def make_blackjack_env():\n",
    "    env = gym.make(\"Blackjack-v1\", natural=True)\n",
    "    return FlattenObservation(env)\n",
    "\n",
    "# Crear el entorno vectorizado\n",
    "env = make_vec_env(make_blackjack_env, n_envs=1)\n",
    "\n",
    "# Inicializar el modelo PPO\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Entrenar el modelo PPO\n",
    "model.learn(total_timesteps=50000)  # Ajusta según los recursos disponibles\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save(\"ppo_blackjack\")\n",
    "print(\"Modelo entrenado y guardado como 'ppo_blackjack'.\")"
   ],
   "metadata": {
    "id": "m9JsFA1wGmnH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2c08c816-285c-4804-d3a9-6a8b048db920"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.48     |\n",
      "|    ep_rew_mean     | -0.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 841      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.26        |\n",
      "|    ep_rew_mean          | -0.48       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 610         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019496009 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.674      |\n",
      "|    explained_variance   | -0.00731    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.278       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0578     |\n",
      "|    value_loss           | 0.77        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.18       |\n",
      "|    ep_rew_mean          | -0.32      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 602        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01741126 |\n",
      "|    clip_fraction        | 0.286      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.628     |\n",
      "|    explained_variance   | 0.0682     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.31       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0425    |\n",
      "|    value_loss           | 0.776      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.26       |\n",
      "|    ep_rew_mean          | -0.24      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 596        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02580231 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.56      |\n",
      "|    explained_variance   | 0.06       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.363      |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0347    |\n",
      "|    value_loss           | 0.798      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.24        |\n",
      "|    ep_rew_mean          | -0.18       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 573         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012490791 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.493      |\n",
      "|    explained_variance   | 0.0991      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.414       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 0.801       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.33        |\n",
      "|    ep_rew_mean          | -0.03       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 575         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007620056 |\n",
      "|    clip_fraction        | 0.0728      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.462      |\n",
      "|    explained_variance   | 0.145       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.387       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00992    |\n",
      "|    value_loss           | 0.752       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.24        |\n",
      "|    ep_rew_mean          | -0.02       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 577         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008071782 |\n",
      "|    clip_fraction        | 0.0766      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.411      |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.389       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    value_loss           | 0.767       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.34         |\n",
      "|    ep_rew_mean          | -0.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 575          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042661233 |\n",
      "|    clip_fraction        | 0.0479       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.389       |\n",
      "|    explained_variance   | 0.125        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.391        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00512     |\n",
      "|    value_loss           | 0.777        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.21         |\n",
      "|    ep_rew_mean          | -0.38        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 564          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029644181 |\n",
      "|    clip_fraction        | 0.0487       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.356       |\n",
      "|    explained_variance   | 0.154        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.388        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0035      |\n",
      "|    value_loss           | 0.757        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.31        |\n",
      "|    ep_rew_mean          | -0.05       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 566         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004791723 |\n",
      "|    clip_fraction        | 0.0475      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.311      |\n",
      "|    explained_variance   | 0.184       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.319       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00436    |\n",
      "|    value_loss           | 0.724       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.45         |\n",
      "|    ep_rew_mean          | -0.02        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 569          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026165135 |\n",
      "|    clip_fraction        | 0.0366       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.284       |\n",
      "|    explained_variance   | 0.183        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.328        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00429     |\n",
      "|    value_loss           | 0.716        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.51        |\n",
      "|    ep_rew_mean          | 0.06        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 564         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004308435 |\n",
      "|    clip_fraction        | 0.0496      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.261      |\n",
      "|    explained_variance   | 0.176       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.448       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00346    |\n",
      "|    value_loss           | 0.729       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.39         |\n",
      "|    ep_rew_mean          | -0.07        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 563          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 47           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023446851 |\n",
      "|    clip_fraction        | 0.039        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.247       |\n",
      "|    explained_variance   | 0.168        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.288        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00203     |\n",
      "|    value_loss           | 0.74         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.46         |\n",
      "|    ep_rew_mean          | -0.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 566          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 50           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021976226 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.246       |\n",
      "|    explained_variance   | 0.223        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.337        |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00056     |\n",
      "|    value_loss           | 0.677        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.37         |\n",
      "|    ep_rew_mean          | -0.03        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 567          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 54           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025346898 |\n",
      "|    clip_fraction        | 0.0337       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.219       |\n",
      "|    explained_variance   | 0.179        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.325        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -7.29e-05    |\n",
      "|    value_loss           | 0.728        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.43        |\n",
      "|    ep_rew_mean          | -0.17       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 561         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002071979 |\n",
      "|    clip_fraction        | 0.0285      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.2        |\n",
      "|    explained_variance   | 0.178       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.349       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    value_loss           | 0.714       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5          |\n",
      "|    ep_rew_mean          | 0.01         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 562          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022704604 |\n",
      "|    clip_fraction        | 0.0269       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.199       |\n",
      "|    explained_variance   | 0.169        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.361        |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000294    |\n",
      "|    value_loss           | 0.734        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.45         |\n",
      "|    ep_rew_mean          | -0.06        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 555          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 66           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041385554 |\n",
      "|    clip_fraction        | 0.0334       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.198       |\n",
      "|    explained_variance   | 0.18         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.353        |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | 0.000558     |\n",
      "|    value_loss           | 0.734        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.33         |\n",
      "|    ep_rew_mean          | -0.05        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 552          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 70           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024115036 |\n",
      "|    clip_fraction        | 0.0327       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.193       |\n",
      "|    explained_variance   | 0.183        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.372        |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | 0.000749     |\n",
      "|    value_loss           | 0.71         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.42         |\n",
      "|    ep_rew_mean          | 0.01         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 553          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 73           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032707083 |\n",
      "|    clip_fraction        | 0.0486       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.175       |\n",
      "|    explained_variance   | 0.142        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.371        |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    value_loss           | 0.726        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.41         |\n",
      "|    ep_rew_mean          | -0.08        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 555          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 77           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033622582 |\n",
      "|    clip_fraction        | 0.0472       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.179       |\n",
      "|    explained_variance   | 0.176        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.337        |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -7.69e-05    |\n",
      "|    value_loss           | 0.719        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.46        |\n",
      "|    ep_rew_mean          | 0.03        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 556         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001782119 |\n",
      "|    clip_fraction        | 0.0287      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.166      |\n",
      "|    explained_variance   | 0.183       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.371       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | 0.000928    |\n",
      "|    value_loss           | 0.722       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5          |\n",
      "|    ep_rew_mean          | -0.08        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 552          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 85           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042306166 |\n",
      "|    clip_fraction        | 0.0388       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.154       |\n",
      "|    explained_variance   | 0.198        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.338        |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.000324    |\n",
      "|    value_loss           | 0.701        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.57         |\n",
      "|    ep_rew_mean          | -0.23        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 553          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 88           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032990775 |\n",
      "|    clip_fraction        | 0.0302       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.159       |\n",
      "|    explained_variance   | 0.178        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.34         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | 0.000584     |\n",
      "|    value_loss           | 0.719        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.51         |\n",
      "|    ep_rew_mean          | -0.12        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 555          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 92           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042101163 |\n",
      "|    clip_fraction        | 0.0468       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.152       |\n",
      "|    explained_variance   | 0.196        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.388        |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000722    |\n",
      "|    value_loss           | 0.705        |\n",
      "------------------------------------------\n",
      "Modelo entrenado y guardado como 'ppo_blackjack'.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **1.1.4 Evaluación de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
   ],
   "metadata": {
    "id": "E-bpdb8wZID1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Asegurarse de que eval_env está definido\n",
    "eval_env = make_vec_env(make_blackjack_env, n_envs=1)\n",
    "\n",
    "# Evaluar el modelo entrenado\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=5000, deterministic=True)\n",
    "\n",
    "print(f\"Recompensa Media del Modelo Entrenado: {mean_reward}\")\n",
    "print(f\"Desviación Estándar del Modelo Entrenado: {std_reward}\")"
   ],
   "metadata": {
    "id": "5-d7d8GFf7F6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d3129a3b-b511-4112-9a94-c9fbbc5c61ec"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Recompensa Media del Modelo Entrenado: -0.0514\n",
      "Desviación Estándar del Modelo Entrenado: 0.9551743505768987\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
    "\n",
    "Genere una función que reciba un estado y retorne la accion del agente. Luego, use esta función para entregar la acción escogida frente a los siguientes escenarios:\n",
    "\n",
    "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
    "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
    "\n",
    "¿Son coherentes sus acciones con las reglas del juego?\n",
    "\n",
    "Hint: ¿A que clase de python pertenecen los estados? Pruebe a usar el método `.reset` para saberlo."
   ],
   "metadata": {
    "id": "RO-EsAaPAYEm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_agent_action(state):\n",
    "    # Convertir el estado en un array de tipo float32\n",
    "    state_array = np.array(state, dtype=np.float32)\n",
    "\n",
    "    # Asegurarnos de que el estado tenga la forma correcta (1, 3)\n",
    "    state_array = state_array.reshape(1, -1)\n",
    "\n",
    "    # Obtener la acción del modelo entrenado\n",
    "    action, _ = model.predict(state_array, deterministic=True)\n",
    "\n",
    "    # Extraer la acción (0: plantarse, 1: pedir carta)\n",
    "    return action[0]\n",
    "\n",
    "# Escenario 1\n",
    "state1 = (6, 7, False)\n",
    "action1 = get_agent_action(state1)\n",
    "print(f\"Acción del agente en el Escenario 1: {'Pedir carta' if action1 == 1 else 'Plantarse'}\")\n",
    "\n",
    "# Escenario 2\n",
    "state2 = (19, 3, True)\n",
    "action2 = get_agent_action(state2)\n",
    "print(f\"Acción del agente en el Escenario 2: {'Pedir carta' if action2 == 1 else 'Plantarse'}\")\n"
   ],
   "metadata": {
    "id": "Fh8XlGyzwtRp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9d7fcd90-53db-424b-d106-da02c876f3af"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acción del agente en el Escenario 1: Pedir carta\n",
      "Acción del agente en el Escenario 2: Plantarse\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) # notar el parámetro continuous = True"
   ],
   "metadata": {
    "id": "nvQUyuZ_FtZ4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9a73aa25-1d17-4a55-c7d2-832bb7e6e914"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Noten que se especifica el parámetro `continuous = True`. ¿Que implicancias tiene esto sobre el ambiente?\n",
    "\n",
    "Además, se le facilita la función `export_gif` para el ejercicio 2.2.4:"
   ],
   "metadata": {
    "id": "FBU4lGX3wpN6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def export_gif(model, n = 5):\n",
    "  '''\n",
    "  función que exporta a gif el comportamiento del agente en n episodios\n",
    "  '''\n",
    "  images = []\n",
    "  for episode in range(n):\n",
    "    obs = model.env.reset()\n",
    "    img = model.env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "      images.append(img)\n",
    "      action, _ = model.predict(obs)\n",
    "      obs, reward, done, info = model.env.step(action)\n",
    "      img = model.env.render(mode=\"rgb_array\")\n",
    "\n",
    "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ],
   "metadata": {
    "id": "bRiWpSo9yfr9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **1.2.1 Descripción de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripción sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. ¿Como se distinguen las acciones de este ambiente en comparación a `Blackjack`?\n",
    "\n",
    "Nota: recuerde que se especificó el parámetro `continuous = True`"
   ],
   "metadata": {
    "id": "sk5VJVppXh3N"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`escriba su respuesta acá`"
   ],
   "metadata": {
    "id": "Yb-u9LUE8O9a"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 10 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política?"
   ],
   "metadata": {
    "id": "YChodtNQwzG2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Crear el entorno LunarLander con acciones continuas\n",
    "env = gym.make(\"LunarLander-v3\", continuous=True)\n",
    "\n",
    "n_episodes = 10\n",
    "total_rewards = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        # Seleccionar una acción aleatoria\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "    total_rewards.append(episode_reward)\n",
    "    print(f\"Recompensa del Episodio {episode + 1}: {episode_reward:.2f}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Calcular estadísticas\n",
    "mean_reward = np.mean(total_rewards)\n",
    "std_reward = np.std(total_rewards)\n",
    "\n",
    "print(f\"\\nRecompensa Media: {mean_reward:.2f}\")\n",
    "print(f\"Desviación Estándar: {std_reward:.2f}\")"
   ],
   "metadata": {
    "id": "5bwc3A0GX7a8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "650dadf5-bede-4d4a-8985-ad90679d4f9b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Recompensa del Episodio 1: -276.31\n",
      "Recompensa del Episodio 2: -48.98\n",
      "Recompensa del Episodio 3: -161.62\n",
      "Recompensa del Episodio 4: -458.87\n",
      "Recompensa del Episodio 5: -109.33\n",
      "Recompensa del Episodio 6: -22.06\n",
      "Recompensa del Episodio 7: -232.30\n",
      "Recompensa del Episodio 8: -107.85\n",
      "Recompensa del Episodio 9: -205.06\n",
      "Recompensa del Episodio 10: -66.31\n",
      "\n",
      "Recompensa Media: -168.87\n",
      "Desviación Estándar: 124.40\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
   ],
   "metadata": {
    "id": "hQrZVQflX_5f"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "# Crear el entorno LunarLander con acciones continuas\n",
    "env = gym.make(\"LunarLander-v3\", continuous=True)\n",
    "\n",
    "# Crear el modelo SAC\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Entrenar el modelo durante 10,000 timesteps\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save(\"sac_lunarlander\")\n",
    "print(\"Modelo entrenado y guardado como 'sac_lunarlander'.\")\n"
   ],
   "metadata": {
    "id": "y_6Ia9uoF7Hs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f32048f4-3ebf-4200-dc9b-5ce3da8d3094"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 113      |\n",
      "|    ep_rew_mean     | -303     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 453      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.85     |\n",
      "|    critic_loss     | 52.2     |\n",
      "|    ent_coef        | 0.903    |\n",
      "|    ent_coef_loss   | -0.293   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 352      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 127      |\n",
      "|    ep_rew_mean     | -268     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 1018     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.58     |\n",
      "|    critic_loss     | 4.43     |\n",
      "|    ent_coef        | 0.769    |\n",
      "|    ent_coef_loss   | -0.698   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 917      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 188      |\n",
      "|    ep_rew_mean     | -206     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 2251     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.2      |\n",
      "|    critic_loss     | 5.76     |\n",
      "|    ent_coef        | 0.553    |\n",
      "|    ent_coef_loss   | -1.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2150     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 188      |\n",
      "|    ep_rew_mean     | -185     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 86       |\n",
      "|    total_timesteps | 3011     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.06     |\n",
      "|    critic_loss     | 6.73     |\n",
      "|    ent_coef        | 0.455    |\n",
      "|    ent_coef_loss   | -1.63    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2910     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 248      |\n",
      "|    ep_rew_mean     | -156     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 145      |\n",
      "|    total_timesteps | 4955     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.89     |\n",
      "|    critic_loss     | 3.69     |\n",
      "|    ent_coef        | 0.27     |\n",
      "|    ent_coef_loss   | -2.31    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4854     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 339      |\n",
      "|    ep_rew_mean     | -142     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 239      |\n",
      "|    total_timesteps | 8141     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.78    |\n",
      "|    critic_loss     | 9.17     |\n",
      "|    ent_coef        | 0.116    |\n",
      "|    ent_coef_loss   | -2.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8040     |\n",
      "---------------------------------\n",
      "Modelo entrenado y guardado como 'sac_lunarlander'.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **1.2.4 Evaluación de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
   ],
   "metadata": {
    "id": "3z-oIUSrlAsY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Cargar el modelo entrenado (si no está ya en memoria)\n",
    "# model = SAC.load(\"sac_lunarlander\")\n",
    "\n",
    "# Crear un entorno de evaluación\n",
    "eval_env = gym.make(\"LunarLander-v3\", continuous=True)\n",
    "\n",
    "n_episodes = 10\n",
    "total_rewards = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    obs, info = eval_env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        # Predecir la acción usando el modelo entrenado\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "    total_rewards.append(episode_reward)\n",
    "    print(f\"Recompensa del Episodio {episode + 1}: {episode_reward:.2f}\")\n",
    "\n",
    "eval_env.close()\n",
    "\n",
    "# Calcular estadísticas\n",
    "mean_reward = np.mean(total_rewards)\n",
    "std_reward = np.std(total_rewards)\n",
    "\n",
    "print(f\"\\nRecompensa Media del Modelo Entrenado: {mean_reward:.2f}\")\n",
    "print(f\"Desviación Estándar: {std_reward:.2f}\")\n"
   ],
   "metadata": {
    "id": "ophyU3KrWrwl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a4299672-5292-46c3-868c-e73df1f2317d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Recompensa del Episodio 1: -155.83\n",
      "Recompensa del Episodio 2: -158.93\n",
      "Recompensa del Episodio 3: -26.95\n",
      "Recompensa del Episodio 4: -103.27\n",
      "Recompensa del Episodio 5: -77.49\n",
      "Recompensa del Episodio 6: -110.86\n",
      "Recompensa del Episodio 7: -46.47\n",
      "Recompensa del Episodio 8: -107.64\n",
      "Recompensa del Episodio 9: -109.01\n",
      "Recompensa del Episodio 10: -159.18\n",
      "\n",
      "Recompensa Media del Modelo Entrenado: -105.56\n",
      "Desviación Estándar: 43.31\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **1.2.5 Optimización de modelo (0.2 puntos)**\n",
    "\n",
    "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente parámetros como:\n",
    "- `total_timesteps`\n",
    "- `learning_rate`\n",
    "- `batch_size`\n",
    "\n",
    "Una vez optimizado el modelo, use la función `export_gif` para estudiar el comportamiento de su agente en la resolución del ambiente y comente sobre sus resultados.\n",
    "\n",
    "Adjunte el gif generado en su entrega (mejor aún si además adjuntan el gif en el markdown)."
   ],
   "metadata": {
    "id": "x6Xw4YHT3P5d"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "# Crear el entorno\n",
    "env = gym.make(\"LunarLander-v3\", continuous=True)\n",
    "\n",
    "# Ajustar hiperparámetros\n",
    "learning_rate = 3e-4\n",
    "batch_size = 256\n",
    "total_timesteps = 100000  # Incrementamos el número de timesteps\n",
    "\n",
    "# Crear el modelo con los nuevos hiperparámetros\n",
    "model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "# Guardar el modelo optimizado\n",
    "model.save(\"sac_lunarlander_optimized\")\n",
    "print(\"Modelo optimizado y guardado como 'sac_lunarlander_optimized'.\")\n"
   ],
   "metadata": {
    "id": "aItYF6sr6F_6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "94a69087-9874-4a17-b4fd-8ecbc79c4cfe"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 107      |\n",
      "|    ep_rew_mean     | -173     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 43       |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 428      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.582   |\n",
      "|    critic_loss     | 43.2     |\n",
      "|    ent_coef        | 0.91     |\n",
      "|    ent_coef_loss   | -0.276   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 327      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 146      |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 1172     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.876   |\n",
      "|    critic_loss     | 26.9     |\n",
      "|    ent_coef        | 0.739    |\n",
      "|    ent_coef_loss   | -0.737   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1071     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 158      |\n",
      "|    ep_rew_mean     | -219     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 1902     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.33     |\n",
      "|    critic_loss     | 30.9     |\n",
      "|    ent_coef        | 0.607    |\n",
      "|    ent_coef_loss   | -1.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1801     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 154      |\n",
      "|    ep_rew_mean     | -201     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 70       |\n",
      "|    total_timesteps | 2468     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.69     |\n",
      "|    critic_loss     | 17.3     |\n",
      "|    ent_coef        | 0.522    |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2367     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 242      |\n",
      "|    ep_rew_mean     | -214     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 142      |\n",
      "|    total_timesteps | 4849     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.715   |\n",
      "|    critic_loss     | 5.56     |\n",
      "|    ent_coef        | 0.295    |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4748     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 289      |\n",
      "|    ep_rew_mean     | -201     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 204      |\n",
      "|    total_timesteps | 6934     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.74    |\n",
      "|    critic_loss     | 15.3     |\n",
      "|    ent_coef        | 0.197    |\n",
      "|    ent_coef_loss   | -0.52    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6833     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 289      |\n",
      "|    ep_rew_mean     | -195     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 239      |\n",
      "|    total_timesteps | 8087     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.79    |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.167    |\n",
      "|    ent_coef_loss   | -0.677   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7986     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 311      |\n",
      "|    ep_rew_mean     | -186     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 294      |\n",
      "|    total_timesteps | 9960     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.93    |\n",
      "|    critic_loss     | 4.33     |\n",
      "|    ent_coef        | 0.136    |\n",
      "|    ent_coef_loss   | -0.0767  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9859     |\n",
      "---------------------------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **2. Large Language Models (4.0 puntos)**\n",
    "\n",
    "En esta sección se enfocarán en habilitar un Chatbot que nos permita responder preguntas útiles a través de LLMs."
   ],
   "metadata": {
    "id": "mPUY-Ktgf2BO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **2.0 Configuración Inicial**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Como siempre, cargamos todas nuestras API KEY al entorno:"
   ],
   "metadata": {
    "id": "mQ4fPRRihGLe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ],
   "metadata": {
    "id": "Ud2Xm_k-hFJn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsección es que habiliten un chatbot que pueda responder preguntas usando información contenida en documentos PDF a través de **Retrieval Augmented Generation.**"
   ],
   "metadata": {
    "id": "Rj9JvQUsgZZJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
    "\n",
    "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
    "  - 2 documentos .pdf como mínimo.\n",
    "  - 50 páginas de contenido como mínimo entre todos los documentos.\n",
    "  - Ideas para documentos: Documentos relacionados a temas académicos, laborales o de ocio. Aprovechen este ejercicio para construir algo útil y/o relevante para ustedes!\n",
    "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
    "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
    "  - **Recuerden adjuntar los documentos en su entrega**."
   ],
   "metadata": {
    "id": "ZrxOQroVnaZ5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install --upgrade --quiet PyPDF2"
   ],
   "metadata": {
    "id": "5D1tIRCi4oJJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import PyPDF2\n",
    "\n",
    "doc_paths = [] # rellenar con los path a sus documentos\n",
    "\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un mínimo de 2 documentos\"\n",
    "\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"Páginas insuficientes: {total_paginas}\""
   ],
   "metadata": {
    "id": "kzq2TjWCnu15"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
    "\n",
    "Vectorice los documentos y almacene sus representaciones de manera acorde."
   ],
   "metadata": {
    "id": "r811-P71nizA"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "n-yXAdCSn4JM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
    "\n",
    "Habilite la solución RAG a través de una *chain* y guárdela en una variable."
   ],
   "metadata": {
    "id": "hAUkP5zrnyBK"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "gPIySdDFn99l"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **2.1.4 Verificación de respuestas (0.5 puntos)**\n",
    "\n",
    "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su solución para cada una. ¿Su solución RAG entrega las respuestas que esperaba?\n",
    "\n",
    "Ejemplo de tupla:\n",
    "- Pregunta: ¿Quién es el presidente de Chile?\n",
    "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
   ],
   "metadata": {
    "id": "ycg5S5i_n-kL"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "S_UiEn1hoZYR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **2.1.5 Sensibilidad de Hiperparámetros (0.5 puntos)**\n",
    "\n",
    "Extienda el análisis del punto 2.1.4 analizando cómo cambian las respuestas entregadas cambiando los siguientes hiperparámetros:\n",
    "- `Tamaño del chunk`. (*¿Cómo repercute que los chunks sean mas grandes o chicos?*)\n",
    "- `La cantidad de chunks recuperados`. (*¿Qué pasa si se devuelven muchos/pocos chunks?*)\n",
    "- `El tipo de búsqueda`. (*¿Cómo afecta el tipo de búsqueda a las respuestas de mi RAG?*)"
   ],
   "metadata": {
    "id": "X8d5zTMHoUgF"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "UDh_QgeXLGHc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **2.2 Agentes (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la sección anterior, en esta sección se busca habilitar **Agentes** para obtener información a través de tools y así responder la pregunta del usuario."
   ],
   "metadata": {
    "id": "ENJiPPM0giX8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas al motor de búsqueda **Tavily**."
   ],
   "metadata": {
    "id": "V47l7Mjfrk0N"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "R6SLKwcWr0AG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
    "\n",
    "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
   ],
   "metadata": {
    "id": "SonB1A-9rtRq"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ehJJpoqsr26-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
    "\n",
    "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Asegúrese que su agente responda en español. Por último, guarde el agente en una variable."
   ],
   "metadata": {
    "id": "CvUIMdX6r0ne"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "pD1_n0wrsDI5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **2.2.4 Verificación de respuestas (0.3 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente y asegúrese que el agente esté ocupando correctamente las tools disponibles. ¿En qué casos el agente debería ocupar la tool de Tavily? ¿En qué casos debería ocupar la tool de Wikipedia?"
   ],
   "metadata": {
    "id": "dKV0JxK3r-XG"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Pqo2dsxvywW_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **2.3 Multi Agente (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
    "\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsección es encapsular las funcionalidades creadas en una solución multiagente con un **supervisor**.\n"
   ],
   "metadata": {
    "id": "cZbDTYiogquv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
    "\n",
    "Transforme la solución RAG de la sección 2.1 y el agente de la sección 2.2 a *tools* (una tool por cada uno)."
   ],
   "metadata": {
    "id": "7-iUfH0WvI6m"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "pw1cfTtvv1AZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
    "\n",
    "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
   ],
   "metadata": {
    "id": "HQYNjT_0vPCg"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "yv2ZY0BAv1RD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **2.3.3 Verificación de respuestas (0.25 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. ¿Cómo varían las respuestas bajo este enfoque?"
   ],
   "metadata": {
    "id": "ea3zWlvyvY7K"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "6_1t0zkgv1qW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **2.3.4 Análisis (0.25 puntos)**\n",
    "\n",
    "¿Qué diferencias tiene este enfoque con la solución *Router* vista en clases? Nombre al menos una ventaja y desventaja."
   ],
   "metadata": {
    "id": "Qb8bdAmYvgwn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`escriba su respuesta acá`"
   ],
   "metadata": {
    "id": "YAUlJxqoLK5r"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
    "\n",
    "- Pregunta 1: \"Hola! mi nombre es Sebastián\"\n",
    "  - Respuesta esperada: \"Hola Sebastián! ...\"\n",
    "- Pregunta 2: \"Cual es mi nombre?\"\n",
    "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
    "  - **Respuesta esperada: \"Tu nombre es Sebastián\"**\n",
    "\n",
    "Para solucionar esto, se les solicita agregar un componente de **memoria** a la solución entregada en el punto 2.3.\n",
    "\n",
    "**Nota: El Bonus es válido <u>sólo para la sección 2 de Large Language Models.</u>**"
   ],
   "metadata": {
    "id": "4JWVSuWiZ8Mj"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "K6Y7tIPJLPfB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **2.5 Despliegue (0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a través de `gradio`, una librería especializada en el levantamiento rápido de demos basadas en ML.\n",
    "\n",
    "Primero instalamos la librería:"
   ],
   "metadata": {
    "id": "vFc3jBT5g0kT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install --upgrade --quiet gradio"
   ],
   "metadata": {
    "id": "T8TsvnCPbkIA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Luego sólo deben ejecutar el siguiente código e interactuar con la interfaz a través del notebook o del link generado:"
   ],
   "metadata": {
    "id": "HJBztEUovKsF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def agent_response(message, history):\n",
    "  '''\n",
    "  Función para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
    "  '''\n",
    "  # get chatbot response\n",
    "  response = ... # rellenar con la respuesta de su chat\n",
    "\n",
    "  # assert\n",
    "  assert type(response) == str, \"output de route_question debe ser string\"\n",
    "\n",
    "  # \"streaming\" response\n",
    "  for i in range(len(response)):\n",
    "    time.sleep(0.015)\n",
    "    yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    agent_response,\n",
    "    type=\"messages\",\n",
    "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
    "    description=\"Hola! Soy un chatbot muy útil :)\", # también la descripción\n",
    "    theme=\"soft\",\n",
    "    ).launch(\n",
    "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
    "        debug = False,\n",
    "        )"
   ],
   "metadata": {
    "id": "Z3KedQSvg1-n"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
